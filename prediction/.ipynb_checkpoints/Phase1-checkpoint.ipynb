{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a42fe2-a703-4827-bdb9-d56d147add33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define model Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d4b23-4617-4387-9499-0b73552ee685",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e1eca9-f35a-4f7e-8b7b-b9b4606c6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "from scipy.special import logsumexp\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.regularizers import l2\n",
    "from keras import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Bidirectional\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e7497-fc54-4e23-b299-8d1223c6f5b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LSTM (Unidirectional)\n",
    "\n",
    "This is the original implementation as used in the paper\n",
    "\n",
    "we rename the model from \"net\" to \"UniLSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19967901-d1d1-4b2a-9758-2467570d2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniLSTM:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X_train, X_train_ctx, y_train, regression, loss, n_epochs = 100,\n",
    "        normalize = False, y_normalize=False, tau = 1.0, dropout = 0.05, batch_size= 128, context=True, num_folds=10, model_name='predictor', checkpoint_dir='./checkpoints/'):\n",
    "\n",
    "        \"\"\"\n",
    "            Constructor for the class implementing a Bayesian neural network\n",
    "            trained with the probabilistic back propagation method.\n",
    "            @param X_train      Matrix with the features for the training data.\n",
    "            @param y_train      Vector with the target variables for the\n",
    "                                training data.\n",
    "            @param n_epochs     Numer of epochs for which to train the\n",
    "                                network. The recommended value 40 should be\n",
    "                                enough.\n",
    "            @param normalize    Whether to normalize the input features. This\n",
    "                                is recommended unles the input vector is for\n",
    "                                example formed by binary features (a\n",
    "                                fingerprint). In that case we do not recommend\n",
    "                                to normalize the features.\n",
    "            @param tau          Tau value used for regularization\n",
    "            @param dropout      Dropout rate for all the dropout layers in the\n",
    "                                network.\n",
    "        \"\"\"\n",
    "\n",
    "        # We normalize the training data to have zero mean and unit standard\n",
    "        # deviation in the training set if necessary\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            self.std_X_train_ctx = np.std(X_train_ctx, 0)\n",
    "            self.std_X_train_ctx[ self.X_train_ctx == 0 ] = 1\n",
    "            self.mean_X_train_ctx = np.mean(X_train_ctx, 0)\n",
    "        else:\n",
    "            self.std_X_train_ctx = np.ones(X_train_ctx.shape[ 1 ])\n",
    "            self.mean_X_train_ctx = np.zeros(X_train_ctx.shape[ 1 ])\n",
    "\n",
    "        X_train_ctx = (X_train_ctx - np.full(X_train_ctx.shape, self.mean_X_train_ctx)) / \\\n",
    "            np.full(X_train_ctx.shape, self.std_X_train_ctx)\n",
    "        \"\"\"\n",
    "        if y_normalize:\n",
    "            self.mean_y_train = np.mean(y_train)\n",
    "            self.std_y_train = np.std(y_train)\n",
    "\n",
    "            y_train_normalized = (y_train - self.mean_y_train) / self.std_y_train\n",
    "            y_train_normalized = np.array(y_train_normalized, ndmin = 2).T\n",
    "        else:\n",
    "            if len(y_train.shape)==1:\n",
    "                y_train_normalized = np.array(y_train, ndmin = 2).T\n",
    "            else:\n",
    "                y_train_normalized = y_train\n",
    "\n",
    "\n",
    "        # We construct the network\n",
    "        N = X_train.shape[0]\n",
    "        batch_size = batch_size\n",
    "        num_folds = num_folds\n",
    "\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]), name='main_input')\n",
    "        inter = Dropout(dropout)(inputs, training=True)\n",
    "        inter = LSTM(30, recurrent_dropout=dropout, return_sequences=True)(inputs, training=True)\n",
    "        inter = Dropout(dropout)(inter, training=True)\n",
    "        inter = LSTM(30)(inter, training=True)\n",
    "        inter = Dropout(dropout)(inter, training=True)\n",
    "\n",
    "        if context==True:\n",
    "            context_shape = X_train_ctx.shape\n",
    "            auxiliary_input = Input(shape=(context_shape[1],), name='aux_input')\n",
    "            aux_inter = Dropout(dropout)(auxiliary_input, training=True)\n",
    "\n",
    "            inter = keras.layers.concatenate([inter, aux_inter])\n",
    "            inter = Dropout(dropout)(inter, training=True)\n",
    "\n",
    "            if regression:\n",
    "                outputs = Dense(y_train_normalized.shape[1], )(inter)\n",
    "            else:\n",
    "                outputs = Dense(y_train_normalized.shape[1], activation='softmax')(inter)\n",
    "            model = Model(inputs=[inputs,auxiliary_input], outputs=outputs)\n",
    "\n",
    "            model.compile(loss=loss, optimizer='adam')\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "            model_checkpoint = keras.callbacks.ModelCheckpoint('%smodel_%s_.h5' % (checkpoint_dir, model_name), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "            lr_reducer = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "            # We iterate the learning process\n",
    "            start_time = time.time()\n",
    "            model.fit([X_train,X_train_ctx], y_train_normalized, batch_size=batch_size, epochs=n_epochs, verbose=2, validation_split=1/num_folds, callbacks=[early_stopping, model_checkpoint, lr_reducer])\n",
    "        else:\n",
    "            if regression:\n",
    "                outputs = Dense(y_train_normalized.shape[1], )(inter)\n",
    "            else:\n",
    "                outputs = Dense(y_train_normalized.shape[1], activation='softmax')(inter)\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "            model.compile(loss=loss, optimizer='adam')\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "            model_checkpoint = keras.callbacks.ModelCheckpoint('%smodel_%s_.h5' % (checkpoint_dir, model_name), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "            lr_reducer = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "            # We iterate the learning process\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train, y_train_normalized, batch_size=batch_size, epochs=n_epochs, verbose=2, validation_split=1/num_folds, callbacks=[early_stopping, model_checkpoint, lr_reducer])\n",
    "\n",
    "        self.model = model\n",
    "        self.tau = tau\n",
    "        self.running_time = time.time() - start_time\n",
    "\n",
    "        # We are done!\n",
    "\n",
    "    def load(self, checkpoint_dir, model_name, loss, compiles):\n",
    "        if not compiles:\n",
    "            model = load_model('%smodel_%s_.h5' % (checkpoint_dir, model_name), compile=compiles)\n",
    "            model.compile(loss=loss, optimizer='adam')\n",
    "        else:\n",
    "            model = load_model('%smodel_%s_.h5' % (checkpoint_dir, model_name))\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X_test, X_test_ctx=None, context=True):\n",
    "\n",
    "        \"\"\"\n",
    "            Function for making predictions with the Bayesian neural network.\n",
    "            @param X_test   The matrix of features for the test data\n",
    "\n",
    "\n",
    "            @return m       The predictive mean for the test target variables.\n",
    "            @return v       The predictive variance for the test target\n",
    "                            variables.\n",
    "            @return v_noise The estimated variance for the additive noise.\n",
    "        \"\"\"\n",
    "\n",
    "        X_test = np.array(X_test, ndmin = 3)\n",
    "\n",
    "\n",
    "        # We normalize the test set\n",
    "        #X_test_ctx = (X_test_ctx - np.full(X_test_ctx.shape, self.mean_X_train_ctx)) /    np.full(X_test_ctx.shape, self.std_X_train_ctx)\n",
    "\n",
    "        # We compute the predictive mean and variance for the target variables\n",
    "        # of the test data\n",
    "\n",
    "        model = self.model\n",
    "        \"\"\"\n",
    "        standard_pred = model.predict([X_test, X_test_ctx], batch_size=500, verbose=1)\n",
    "        standard_pred = standard_pred * self.std_y_train + self.mean_y_train\n",
    "        rmse_standard_pred = np.mean((y_test.squeeze() - standard_pred.squeeze())**2.)**0.5\n",
    "        \"\"\"\n",
    "        T = 10\n",
    "        #if context==True:\n",
    "        X_test_ctx = np.array(X_test_ctx, ndmin=2)\n",
    "        Yt_hat = np.array([model.predict([X_test, X_test_ctx], batch_size=1, verbose=0) for _ in range(T)])\n",
    "        #else:\n",
    "        #    Yt_hat = np.array([model.predict(X_test, batch_size=1, verbose=0) for _ in range(T)])\n",
    "        #Yt_hat = Yt_hat * self.std_y_train + self.mean_y_train\n",
    "        regression=False\n",
    "        if regression:\n",
    "            MC_pred = np.mean(Yt_hat, 0)\n",
    "            MC_uncertainty = np.std(Yt_hat, 0)\n",
    "        else:\n",
    "            MC_pred = np.mean(Yt_hat, 0)\n",
    "            MC_uncertainty = list()\n",
    "            for i in range(Yt_hat.shape[2]):\n",
    "                MC_uncertainty.append(np.std(Yt_hat[:,:,i].squeeze(),0))\n",
    "        #rmse = np.mean((y_test.squeeze() - MC_pred.squeeze())**2.)**0.5\n",
    "\n",
    "        # We compute the test log-likelihood\n",
    "        \"\"\"\n",
    "        ll = (logsumexp(-0.5 * self.tau * (y_test[None] - Yt_hat)**2., 0) - np.log(T)\n",
    "            - 0.5*np.log(2*np.pi) + 0.5*np.log(self.tau))\n",
    "        test_ll = np.mean(ll)\n",
    "        \"\"\"\n",
    "        # We are done!\n",
    "        return MC_pred, MC_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76081b-59fd-488c-96a0-c8de2c700a9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bi-directional LSTM\n",
    "\n",
    "The code for this is based on the original code, I have only changed the part where models are described.\n",
    "Also, a bug was fixed in model.save and model.load as well as in predict function (for with contextual information part)\n",
    "\n",
    "We name the model \"BiLSTM\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f23eb1-4857-402d-8aae-d613447916e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X_train, X_train_ctx, y_train, regression, loss, n_epochs=100,\n",
    "              normalize=False, y_normalize=False, tau=1.0, dropout=0.05, batch_size=128, context=True,\n",
    "              num_folds=10, model_name='predictor', checkpoint_dir='./checkpoints/'):\n",
    "\n",
    "        if y_normalize:\n",
    "            self.mean_y_train = np.mean(y_train)\n",
    "            self.std_y_train = np.std(y_train)\n",
    "\n",
    "            y_train_normalized = (y_train - self.mean_y_train) / self.std_y_train\n",
    "            y_train_normalized = np.array(y_train_normalized, ndmin=2).T\n",
    "        else:\n",
    "            if len(y_train.shape) == 1:\n",
    "                y_train_normalized = np.array(y_train, ndmin=2).T\n",
    "            else:\n",
    "                y_train_normalized = y_train\n",
    "\n",
    "        # We construct the network\n",
    "        N = X_train.shape[0]\n",
    "        batch_size = batch_size\n",
    "        num_folds = num_folds\n",
    "\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]), name='main_input')\n",
    "        inter = Dropout(dropout)(inputs, training=True)\n",
    "        inter = Bidirectional(LSTM(64, recurrent_dropout=dropout, return_sequences=True))(inter, training=True)\n",
    "        inter = Dropout(dropout)(inter, training=True)\n",
    "        inter = Bidirectional(LSTM(64))(inter, training=True)\n",
    "        inter = Dropout(dropout)(inter, training=True)\n",
    "\n",
    "        if context == True:\n",
    "            context_shape = X_train_ctx.shape\n",
    "            auxiliary_input = Input(shape=(context_shape[1],), name='aux_input')\n",
    "            aux_inter = Dropout(dropout)(auxiliary_input, training=True)\n",
    "\n",
    "            inter = keras.layers.concatenate([inter, aux_inter])\n",
    "            inter = Dropout(dropout)(inter, training=True)\n",
    "\n",
    "            if regression:\n",
    "                outputs = Dense(y_train_normalized.shape[1])(inter)\n",
    "            else:\n",
    "                outputs = Dense(y_train_normalized.shape[1], activation='softmax')(inter)\n",
    "            model = Model(inputs=[inputs, auxiliary_input], outputs=outputs)\n",
    "\n",
    "            model.compile(loss=loss, optimizer='adam')\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "            model_checkpoint = keras.callbacks.ModelCheckpoint('%smodel_%s_.h5' % (checkpoint_dir, model_name),\n",
    "                                                               monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                                                               save_weights_only=False, mode='auto')\n",
    "            lr_reducer = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0,\n",
    "                                                            mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "            # We iterate the learning process\n",
    "            start_time = time.time()\n",
    "            model.fit([X_train, X_train_ctx], y_train_normalized, batch_size=batch_size, epochs=n_epochs,\n",
    "                      verbose=2, validation_split=1 / num_folds, callbacks=[early_stopping, model_checkpoint, lr_reducer])\n",
    "        else:\n",
    "            if regression:\n",
    "                outputs = Dense(y_train_normalized.shape[1])(inter)\n",
    "            else:\n",
    "                outputs = Dense(y_train_normalized.shape[1], activation='softmax')(inter)\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "            model.compile(loss=loss, optimizer='adam')\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "            model_checkpoint = keras.callbacks.ModelCheckpoint('%smodel_%s_.h5' % (checkpoint_dir, model_name),\n",
    "                                                               monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                                                               save_weights_only=False, mode='auto')\n",
    "            lr_reducer = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0,\n",
    "                                                            mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "            # We iterate the learning process\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train, y_train_normalized, batch_size=batch_size, epochs=n_epochs,\n",
    "                      verbose=2, validation_split=1 / num_folds, callbacks=[early_stopping, model_checkpoint, lr_reducer])\n",
    "\n",
    "        self.model = model\n",
    "        self.tau = tau\n",
    "        self.running_time = time.time() - start_time\n",
    "\n",
    "        # We are done!\n",
    "\n",
    "    def load(self, checkpoint_dir, model_name, loss, compiles):\n",
    "        if not compiles:\n",
    "            model = load_model('%smodel_%s_.h5' % (checkpoint_dir, model_name), compile=compiles)\n",
    "            model.compile(loss=loss, optimizer='adam')\n",
    "        else:\n",
    "            model = load_model('%smodel_%s_.h5' % (checkpoint_dir, model_name))\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X_test, X_test_ctx=None, context=True):\n",
    "        X_test = np.array(X_test, ndmin=3)\n",
    "\n",
    "        model = self.model\n",
    "\n",
    "        T = 10\n",
    "        #if context==True:\n",
    "        X_test_ctx = np.array(X_test_ctx, ndmin=2)\n",
    "        Yt_hat = np.array([model.predict([X_test, X_test_ctx], batch_size=1, verbose=0) for _ in range(T)])\n",
    "        # if context == True:\n",
    "        #     X_test_ctx = np.array(X_test_ctx, ndmin=2)\n",
    "        #     Yt_hat = np.array([model.predict([X_test, X_test_ctx], batch_size=1, verbose=0) for _ in range(T)])\n",
    "        # else:\n",
    "        #     Yt_hat = np.array([model.predict(X_test, batch_size=1, verbose=0) for _ in range(T)])\n",
    "        # Yt_hat = Yt_hat * self.std_y_train + self.mean_y_train\n",
    "        regression = False\n",
    "        if regression:\n",
    "            MC_pred = np.mean(Yt_hat, 0)\n",
    "            MC_uncertainty = np.std(Yt_hat, 0)\n",
    "        else:\n",
    "            MC_pred = np.mean(Yt_hat, 0)\n",
    "            MC_uncertainty = list()\n",
    "            for i in range(Yt_hat.shape[2]):\n",
    "                MC_uncertainty.append(np.std(Yt_hat[:, :, i].squeeze(), 0))\n",
    "\n",
    "        return MC_pred, MC_uncertainty\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560e98b-f2fb-4296-9c3f-a28b3207fb3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Transformer based model\n",
    "\n",
    "We name the model \"TransformerModel\" here."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c22fe159-0eac-413c-9f50-01683930a89b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Transformer(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Register the custom Transformer layer\n",
    "get_custom_objects().update({\"Transformer\": Transformer})\n",
    "\n",
    "class TransformerModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X_train, X_train_ctx, y_train, regression, loss, n_epochs=100,\n",
    "              normalize=False, y_normalize=False, tau=1.0, dropout=0.05, batch_size=128, context=True,\n",
    "              num_folds=10, model_name='predictor', checkpoint_dir='./checkpoints/'):\n",
    "\n",
    "        if y_normalize:\n",
    "            self.mean_y_train = np.mean(y_train)\n",
    "            self.std_y_train = np.std(y_train)\n",
    "\n",
    "            y_train_normalized = (y_train - self.mean_y_train) / self.std_y_train\n",
    "            y_train_normalized = np.array(y_train_normalized, ndmin=2).T\n",
    "        else:\n",
    "            if len(y_train.shape) == 1:\n",
    "                y_train_normalized = np.array(y_train, ndmin=2).T\n",
    "            else:\n",
    "                y_train_normalized = y_train\n",
    "\n",
    "        # We construct the network\n",
    "        N = X_train.shape[0]\n",
    "        batch_size = batch_size\n",
    "        num_folds = num_folds\n",
    "\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]), name='main_input')\n",
    "        inter = Dropout(dropout)(inputs, training=True)\n",
    "        inter = Transformer(embed_dim=X_train.shape[2], num_heads=6, ff_dim=64, rate=dropout)(inter, training=True)\n",
    "        inter = Dropout(dropout)(inter, training=True)\n",
    "\n",
    "        if context:\n",
    "            context_shape = X_train_ctx.shape\n",
    "            auxiliary_input = Input(shape=(context_shape[1],), name='aux_input')\n",
    "            aux_inter = Dropout(dropout)(auxiliary_input, training=True)\n",
    "            aux_inter = tf.expand_dims(aux_inter, axis=1)\n",
    "            aux_inter = tf.tile(aux_inter, [1, X_train.shape[1], 1])  # Reshape to match sequence length\n",
    "            inter = tf.concat([inter, aux_inter], axis=-1)\n",
    "            inter = Dropout(dropout)(inter, training=True)\n",
    "\n",
    "            if regression:\n",
    "                outputs = Dense(y_train_normalized.shape[1])(inter)\n",
    "            else:\n",
    "                outputs = Dense(y_train_normalized.shape[1], activation='softmax')(inter)\n",
    "            model = Model(inputs=[inputs, auxiliary_input], outputs=outputs)\n",
    "\n",
    "            model.compile(loss=loss, optimizer='adam')\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "            model_checkpoint = ModelCheckpoint('%smodel_%s_.h5' % (checkpoint_dir, model_name),\n",
    "                                               monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                                               save_weights_only=False, mode='auto')\n",
    "            lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0,\n",
    "                                            mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "            # We iterate the learning process\n",
    "            start_time = time.time()\n",
    "            model.fit([X_train, X_train_ctx], y_train_normalized, batch_size=batch_size, epochs=n_epochs,\n",
    "                      verbose=2, validation_split=1 / num_folds, callbacks=[early_stopping, model_checkpoint, lr_reducer])\n",
    "        else:\n",
    "            if regression:\n",
    "                outputs = Dense(y_train_normalized.shape[1])(inter)\n",
    "            else:\n",
    "                outputs = Dense(y_train_normalized.shape[1], activation='softmax')(inter)\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "            model.compile(loss=loss, optimizer='adam')\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "            model_checkpoint = ModelCheckpoint('%smodel_%s_.h5' % (checkpoint_dir, model_name),\n",
    "                                               monitor='val_loss', verbose=0, save_best_only=True,\n",
    "                                               save_weights_only=False, mode='auto')\n",
    "            lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0,\n",
    "                                            mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "            # We iterate the learning process\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train, y_train_normalized, batch_size=batch_size, epochs=n_epochs,\n",
    "                      verbose=2, validation_split=1 / num_folds, callbacks=[early_stopping, model_checkpoint, lr_reducer])\n",
    "\n",
    "        self.model = model\n",
    "        self.tau = tau\n",
    "        self.running_time = time.time() - start_time\n",
    "\n",
    "    def load(self, checkpoint_dir, model_name, loss, compiles):\n",
    "        if not compiles:\n",
    "            model = load_model('%smodel_%s_.h5' % (checkpoint_dir, model_name), compile=compiles)\n",
    "            model.compile(loss=loss, optimizer='adam')\n",
    "        else:\n",
    "            model = load_model('%smodel_%s_.h5' % (checkpoint_dir, model_name))\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def predict(self, X_test, X_test_ctx=None, context=True):\n",
    "        X_test = np.array(X_test, ndmin=3)\n",
    "\n",
    "        model = self.model\n",
    "\n",
    "        T = 10\n",
    "#        if context == True:\n",
    "        X_test_ctx = np.array(X_test_ctx, ndmin=2)\n",
    "        Yt_hat = np.array([model.predict([X_test, X_test_ctx], batch_size=1, verbose=0) for _ in range(T)])\n",
    "#        else:\n",
    "#            Yt_hat = np.array([model.predict(X_test, batch_size=1, verbose=0) for _ in range(T)])\n",
    "\n",
    "        regression = False\n",
    "        if regression:\n",
    "            MC_pred = np.mean(Yt_hat, 0)\n",
    "            MC_uncertainty = np.std(Yt_hat, 0)\n",
    "        else:\n",
    "            MC_pred = np.mean(Yt_hat, 0)\n",
    "            MC_uncertainty = list()\n",
    "            for i in range(Yt_hat.shape[2]):\n",
    "                MC_uncertainty.append(np.std(Yt_hat[:, :, i].squeeze(), 0))\n",
    "\n",
    "        return MC_pred, MC_uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbccb6b-208f-49c1-818d-4fca3c428ed1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Model on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ebbcf-30d7-47c4-807a-0458e997b66c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca29982-8823-4e11-b231-40bf9b2ad45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generator import FeatureGenerator\n",
    "\n",
    "import keras\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53e934-a28c-414a-8d8d-4ed2dfe9f5d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define variables (parameters) for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b18fc2a-5e56-44c3-8919-e102e0d3c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_values = list()\n",
    "accuracy_sum = 0.0\n",
    "accuracy_value = 0.0\n",
    "precision_values = list()\n",
    "precision_sum = 0.0\n",
    "precision_value = 0.0\n",
    "recall_values = list()\n",
    "recall_sum = 0.0\n",
    "recall_value = 0.0\n",
    "f1_values = list()\n",
    "f1_sum = 0.0\n",
    "f1_value = 0.0\n",
    "training_time_seconds = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e396efd6-aba9-4cae-96e1-87af9b999add",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"next_timestamp\"\n",
    "contextual_info= True\n",
    "inter_case_level='Level1'\n",
    "\n",
    "# dnn\n",
    "num_epochs=20\n",
    "learning_rate=0.002\n",
    "num_folds=10\n",
    "batch_size_train=256\n",
    "batch_size_test=1\n",
    "\n",
    "# data\n",
    "data_set = \"modi_BPI_2012_dropna_filter_act.csv\"\n",
    "data_dir = \"../sample_data/real/\"\n",
    "checkpoint_dir = \"./estimation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f344d5-1b35-43fa-b48e-2dfb62fe0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = inter_case_level\n",
    "\n",
    "filename = data_dir + data_set\n",
    "model_name = data_set + task + '_bilstm'\n",
    "\n",
    "contextual_info = contextual_info\n",
    "if task == 'next_activity':\n",
    "    loss = 'categorical_crossentropy'\n",
    "    regression = False\n",
    "elif task == 'next_timestamp':\n",
    "    loss = 'mae'\n",
    "    regression = True\n",
    "\n",
    "batch_size = batch_size_train\n",
    "num_folds = num_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb51331-7243-4e44-8954-9a3329b28a2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1a3e5a0-bcfa-45e5-b349-64db952bb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load data\n",
    "FG = FeatureGenerator()\n",
    "df = FG.create_initial_log(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6052fdd8-e9a1-48bd-8323-b6bb832e0973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_char_to_int: {'W-Afhandelen leads-START': 0, 'W-Beoordelen fraude-START': 1, 'W-Completeren aanvraag-START': 2, 'W-Nabellen incomplete dossiers-START': 3, 'W-Nabellen offertes-START': 4, 'W-Valideren aanvraag-START': 5, '!': 6}\n",
      "act_int_to_char: {0: 'W-Afhandelen leads-START', 1: 'W-Beoordelen fraude-START', 2: 'W-Completeren aanvraag-START', 3: 'W-Nabellen incomplete dossiers-START', 4: 'W-Nabellen offertes-START', 5: 'W-Valideren aanvraag-START', 6: '!'}\n",
      "res_char_to_int: {'10138': 0, '10188': 1, '10228': 2, '10609': 3, '10629': 4, '10789': 5, '10809': 6, '10861': 7, '10863': 8, '10881': 9, '10889': 10, '10899': 11, '10909': 12, '10910': 13, '10912': 14, '10913': 15, '10914': 16, '10929': 17, '10931': 18, '10932': 19, '10933': 20, '10935': 21, '10939': 22, '10972': 23, '10982': 24, '11000': 25, '11002': 26, '11003': 27, '11009': 28, '11019': 29, '11049': 30, '11119': 31, '11121': 32, '11122': 33, '11169': 34, '11179': 35, '11180': 36, '11181': 37, '11189': 38, '11201': 39, '11203': 40, '11259': 41, '11289': 42, '11299': 43, '11300': 44, '11302': 45, '11309': 46, '11319': 47}\n",
      "res_int_to_char: {0: 10138, 1: 10188, 2: 10228, 3: 10609, 4: 10629, 5: 10789, 6: 10809, 7: 10861, 8: 10863, 9: 10881, 10: 10889, 11: 10899, 12: 10909, 13: 10910, 14: 10912, 15: 10913, 16: 10914, 17: 10929, 18: 10931, 19: 10932, 20: 10933, 21: 10935, 22: 10939, 23: 10972, 24: 10982, 25: 11000, 26: 11002, 27: 11003, 28: 11009, 29: 11019, 30: 11049, 31: 11119, 32: 11121, 33: 11122, 34: 11169, 35: 11179, 36: 11180, 37: 11181, 38: 11189, 39: 11201, 40: 11203, 41: 11259, 42: 11289, 43: 11299, 44: 11300, 45: 11302, 46: 11309, 47: 11319}\n",
      "(52831, 20, 55) (52831, 7) (52831,)\n"
     ]
    }
   ],
   "source": [
    "#split train and test\n",
    "#train_df, test_df = FG.train_test_split(df, 0.7, 0.3)\n",
    "train_df = df\n",
    "test_df = train_df\n",
    "#create train\n",
    "train_df = FG.order_csv_time(train_df)\n",
    "train_df = FG.queue_level(train_df)\n",
    "train_df.to_csv('./training_data.csv')\n",
    "state_list = FG.get_states(train_df)\n",
    "train_X, train_Y_Event, train_Y_Time = FG.one_hot_encode_history(train_df, checkpoint_dir+data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aad73f-9acf-4b47-9a7e-bc6de63bee15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72a509a2-fe98-4be0-adf5-644d7d1230d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "186/186 - 9s - loss: 18.9205 - val_loss: 15.3555 - lr: 0.0010 - 9s/epoch - 50ms/step\n",
      "Epoch 2/100\n",
      "186/186 - 6s - loss: 13.6624 - val_loss: 11.5565 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 3/100\n",
      "186/186 - 6s - loss: 11.7506 - val_loss: 9.9338 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 4/100\n",
      "186/186 - 6s - loss: 11.0727 - val_loss: 9.5212 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 5/100\n",
      "186/186 - 6s - loss: 10.8623 - val_loss: 9.4097 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 6/100\n",
      "186/186 - 6s - loss: 10.7937 - val_loss: 9.3526 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 7/100\n",
      "186/186 - 6s - loss: 10.7537 - val_loss: 9.3192 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 8/100\n",
      "186/186 - 6s - loss: 10.7279 - val_loss: 9.2738 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 9/100\n",
      "186/186 - 6s - loss: 10.6969 - val_loss: 9.2403 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 10/100\n",
      "186/186 - 6s - loss: 10.6755 - val_loss: 9.2238 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 11/100\n",
      "186/186 - 6s - loss: 10.6374 - val_loss: 9.2080 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 12/100\n",
      "186/186 - 6s - loss: 10.6285 - val_loss: 9.1819 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 13/100\n",
      "186/186 - 6s - loss: 10.6116 - val_loss: 9.1610 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 14/100\n",
      "186/186 - 6s - loss: 10.5986 - val_loss: 9.1808 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 15/100\n",
      "186/186 - 6s - loss: 10.5910 - val_loss: 9.1289 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 16/100\n",
      "186/186 - 6s - loss: 10.5748 - val_loss: 9.1230 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 17/100\n",
      "186/186 - 6s - loss: 10.5686 - val_loss: 9.1354 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 18/100\n",
      "186/186 - 6s - loss: 10.5613 - val_loss: 9.1362 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 19/100\n",
      "186/186 - 6s - loss: 10.5517 - val_loss: 9.1107 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 20/100\n",
      "186/186 - 6s - loss: 10.5416 - val_loss: 9.1022 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 21/100\n",
      "186/186 - 6s - loss: 10.5417 - val_loss: 9.1121 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 22/100\n",
      "186/186 - 6s - loss: 10.5307 - val_loss: 9.0941 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 23/100\n",
      "186/186 - 6s - loss: 10.5244 - val_loss: 9.0936 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 24/100\n",
      "186/186 - 6s - loss: 10.5159 - val_loss: 9.0876 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 25/100\n",
      "186/186 - 6s - loss: 10.5059 - val_loss: 9.0933 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 26/100\n",
      "186/186 - 6s - loss: 10.5055 - val_loss: 9.1311 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 27/100\n",
      "186/186 - 6s - loss: 10.4974 - val_loss: 9.1084 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 28/100\n",
      "186/186 - 6s - loss: 10.4949 - val_loss: 9.1539 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 29/100\n",
      "186/186 - 6s - loss: 10.4825 - val_loss: 9.1059 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 30/100\n",
      "186/186 - 6s - loss: 10.4774 - val_loss: 9.1000 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 31/100\n",
      "186/186 - 6s - loss: 10.4780 - val_loss: 9.1363 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 32/100\n",
      "186/186 - 6s - loss: 10.4731 - val_loss: 9.1060 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 33/100\n",
      "186/186 - 6s - loss: 10.4649 - val_loss: 9.0871 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 34/100\n",
      "186/186 - 6s - loss: 10.4625 - val_loss: 9.1158 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 35/100\n",
      "186/186 - 6s - loss: 10.4593 - val_loss: 9.1283 - lr: 0.0010 - 6s/epoch - 32ms/step\n",
      "Epoch 36/100\n",
      "186/186 - 6s - loss: 10.4572 - val_loss: 9.1187 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 37/100\n",
      "186/186 - 6s - loss: 10.4508 - val_loss: 9.1085 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 38/100\n",
      "186/186 - 6s - loss: 10.4414 - val_loss: 9.1175 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 39/100\n",
      "186/186 - 6s - loss: 10.4434 - val_loss: 9.1382 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 40/100\n",
      "186/186 - 6s - loss: 10.4359 - val_loss: 9.1597 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 41/100\n",
      "186/186 - 6s - loss: 10.4380 - val_loss: 9.1342 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 42/100\n",
      "186/186 - 6s - loss: 10.4347 - val_loss: 9.1512 - lr: 0.0010 - 6s/epoch - 31ms/step\n",
      "Epoch 43/100\n",
      "186/186 - 6s - loss: 10.4326 - val_loss: 9.1484 - lr: 0.0010 - 6s/epoch - 31ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if contextual_info:\n",
    "    train_context_X = FG.generate_context_feature(train_df,state_list)\n",
    "    model = UniLSTM()\n",
    "    if regression:\n",
    "        model.train(train_X, train_context_X, train_Y_Time, regression, loss, batch_size=batch_size, num_folds=num_folds, model_name=model_name, checkpoint_dir=checkpoint_dir)\n",
    "    else:\n",
    "        model.train(train_X, train_context_X, train_Y_Event, regression, loss, batch_size=batch_size, num_folds=num_folds, model_name=model_name, checkpoint_dir=checkpoint_dir)\n",
    "else:\n",
    "    model_name += '_no_context_'\n",
    "    train_context_X = None\n",
    "    model = UniLSTM()\n",
    "    if regression:\n",
    "        model.train(train_X, train_context_X, train_Y_Time, regression, loss, batch_size=batch_size, num_folds=num_folds, model_name=model_name, checkpoint_dir=checkpoint_dir, context=contextual_info)\n",
    "    else:\n",
    "        model.train(train_X, train_context_X, train_Y_Event, regression, loss, batch_size=batch_size, num_folds=num_folds, model_name=model_name, checkpoint_dir=checkpoint_dir, context=contextual_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "912f0c9a-5144-44fb-9e29-ebb647a428e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y_Event, test_Y_Time = train_X, train_Y_Event, train_Y_Time\n",
    "test_context_X = train_context_X\n",
    "test_X = test_X[500]\n",
    "test_context_X = test_context_X[500]\n",
    "test_Y_Event = test_Y_Event[500]\n",
    "MC_pred, MC_uncertainty = model.predict(test_X, test_context_X, test_Y_Event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76a5f7a6-1749-4bfd-82d9-55c7f636b6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.1467607e-04, 2.5164601e-04, 5.3316295e-01, 2.1068042e-04,\n",
       "         2.8287265e-01, 8.0838129e-03, 1.7520364e-01]], dtype=float32),\n",
       " [0.00016987501,\n",
       "  0.0002794283,\n",
       "  0.047618296,\n",
       "  0.00012617663,\n",
       "  0.04839888,\n",
       "  0.0067172125,\n",
       "  0.021131782])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC_pred, MC_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5992b7-6167-4e2e-8e89-455dcee949a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b777779-6d1d-4c97-8570-bd52c51c7357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
